---
title: "Modeling Assignment 1, Hendrik's benchmark"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Setup

```{r}
# add your libraries
library(tidyverse)
library(caret)
library(rpart)
library(tidytext)
library(SnowballC)
library(scales)
library(janeaustenr)

wine = read_rds("../pinot.rds") 
```


## Feature Engineering

```{r}
# create some cool features. Make sure you add comments so I know what you are trying to accomplish!

wine_words <- function(df, j = 1000, stem=F){ 
  # library(tidytext)
  # library(SnowballC)
  data(stop_words)

  words <- df %>%
    unnest_tokens(word, description) %>%
    anti_join(stop_words) %>% # get rid of stop words
    filter(!(word %in% c("wine","pinot","vineyard")))
  
  if(stem){
    words <- words %>% 
      mutate(word = wordStem(word))
  }
  
  words <- words %>% 
    count(id, word) %>% 
    group_by(id) %>% 
    mutate(exists = (n>0)) %>% 
    ungroup %>% 
    group_by(word) %>% 
    mutate(total = sum(n)) %>% 
    filter(total > j) %>% 
    pivot_wider(id_cols = id, names_from = word, values_from = exists, values_fill = list(exists=0)) %>% 
    right_join(select(df,id,province,price,points)) %>% #added price and points
    select(-id) %>% 
    mutate(across(-province, ~replace_na(.x, F)))
}

wino <- wine_words(wine, j=100, stem=F)# Changed j to 100

```



## Jesse added NY to Clai complare for standout NY words
```{r}
# library(scales)
# library(janeaustenr)
wtxt <- wine %>%
unnest_tokens(word, description) %>%
anti_join(stop_words) %>%
filter(str_detect(string = word, pattern = "[a-z+]")) %>% # get rid weird non alphas
filter(str_length(word)>3) %>% # get rid of strings shorter than 3 characters
group_by(word) %>%
mutate(total=n()) %>%
ungroup()
wtxt %>%
filter(province=="New_York" | province=="California") %>%
filter(!(word %in% c("wine","pinot","drink","noir","vineyard","palate","notes","flavors","bottling"))) %>% 
filter(total < 100) %>%
group_by(province, word) %>%
count() %>%
group_by(province) %>%
mutate(proportion = n / sum(n)) %>%
pivot_wider(id_cols = word, names_from = province, values_from = proportion) %>%
ggplot(aes(x = New_York, y = California, color = abs(New_York - California))) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
theme(legend.position="none") +
labs(x = "New York", y = "California", title = "Words describing Pinot Noir from California and Oregon")

```

```{r}
# Words in NY not in Cali
ny <- select(words2, c(feathery, penetrating, island, cutting, loads, 
               drying, astringent, lend, bramble, cassis, province))

```


## Specification

```{r}
# specify the model to be used (i.e. KNN or Naive Bayes) and the tuning parameters used

ctrl <- trainControl(method = "cv", number = 5)
set.seed(504) 

wine_index <- createDataPartition(ny$province, p = 0.80, list = FALSE)
train <- ny[ wine_index, ]
test <- ny[-wine_index, ]

fit <- train(province ~ .,
             data = train, 
             method = "knn",# Changed to knn
             trControl = ctrl,
             tuneLength = 30,# Added tune length @15
             metric = "Kappa")

confusionMatrix(predict(fit, test),factor(test$province))
```


## Best model

```{r}
# Here are a few lines to inspect your best model. Add some comments about optimal hyperparameters.
print(fit)
print(fit$bestTune)
```


## Re-fit and evaluation

```{r}
# the "method" below should match the one you chose above. 

set.seed(1504) # I will choose a different seed for evaluation

wine_index <- createDataPartition(wino$province, p = 0.80, list = FALSE)
train <- wino[ wine_index, ]
test <- wino[-wine_index, ]

# example spec for knn
fit_final <- train(province ~ .,
             data = train, 
             method = "knn",# changed to knn
             tuneGrid=fit$bestTune) 
# The last line means we will fit a model using the best tune parameters your CV found above.

confusionMatrix(predict(fit_final, test),factor(test$province))
```

